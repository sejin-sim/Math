{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_6)_조건부기댓값과 예측 문제.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNNl44dVOEusmawY+6oK97E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sejin-sim/Math/blob/main/2_6)_%EC%A1%B0%EA%B1%B4%EB%B6%80%EA%B8%B0%EB%8C%93%EA%B0%92%EA%B3%BC_%EC%98%88%EC%B8%A1_%EB%AC%B8%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0EiPTGtjTRn"
      },
      "source": [
        "### 1) 조건부기댓값\r\n",
        "1. 조건부기댓값(conditional expectation) or 조건부평균(conditional mean) :\r\n",
        "확률변수 $Y$의 기댓값을 구할 때 주변 확률밀도함수 $p_Y(y)$를 사용하여 가중치를 계산하지 않고 조건부 확률밀도함수 $p_{Y\\vert X}(y|x)$를 이용하여 가중치를 계산\r\n",
        " > $ \\text{E}_Y[Y \\vert X] \r\n",
        "= \\int_{y=-\\infty}^{y=\\infty} y \\, p_{Y \\vert X}(y|x) dy $   \r\n",
        " > 간단히,    \r\n",
        " > $ \\text{E}[Y \\vert X] = \\int y \\, p(y|x) dy $\r\n",
        "\r\n",
        "2. 조건부기댓값에서 조건이 되는 확률변수 $X$의 값 $x$ : 조건부기댓값을 사용하는 사용자가 지정해야 하는 독립변수\r\n",
        "3. 조건부기댓값 : 조건이 되는 확률변수의 값에 따라서 값이 달라지는 확률변수\r\n",
        "4. $\\text{E}[Y \\vert X]$는 조건이 되는 확률변수 $X$의 값 $x$를 입력으로 가지는 함수\r\n",
        "> $ \\text{E}[Y \\vert X=x] = f(x) $   \r\n",
        "> 간단히,   \r\n",
        "> $ \\text{E}[Y \\vert x] = f(x) $\r\n",
        "> * $f(x)$는 조건이 되는 확률변수 $X$의 값 $x$를 입력받아서 결과가 되는 확률변수 $Y$의 기댓값을 출력하는 함수다. \r\n",
        "\r\n",
        "<br/>\r\n",
        "\r\n",
        "### 2) 예측문제\r\n",
        "1. 예측(prediction) 문제 : 두 확률변수가 $X, Y$에서 $X$의 값을 알고 있을 때 $Y$의 값을 알아내는 것\r\n",
        "2. $Y$가 연속확률변수면 회귀분석(regression analysis), $Y$가 이산확률변수면 분류(classification)\r\n",
        "3. $X$의 값 $x$을 알면 조건부확률분포 $P(y|x)$의 분포를 알 수 있지만 가장 대표성이 있는 하나의 값이 되어야하므로 일반적으로 조건부확률분포의 기댓값인 조건부기댓값을 예측문제의 답으로 하는 경우가 많다.\r\n",
        "4. 경우에 따라서는 예측문제의 답으로 중앙값이나 최빈값 등을 계산할 수도 있다. 예측문제의 답은 $\\hat{y}$기호로 표기하기도 한다.\r\n",
        "5. 조건부기댓값은 $x$값의 함수이므로 이 함수를 구할 수 있으면 모든 $x$값에 대한 예측결과를 구한 것과 같다.\r\n",
        "> $x \\xrightarrow {\\text{예측}} \\hat{y} = E[y|x] = f(x)$\r\n",
        "\r\n",
        "<br/>\r\n",
        "\r\n",
        "### 3) 조건부기댓값의 성질\r\n",
        "1. 조건부기댓값 $\\text{E}[Y \\vert X]$가 $X$의 함수, 즉 변환(transform)이므로 **조건부기댓값 $\\text{E}[Y \\vert X]$도 확률변수**다.\r\n",
        "\r\n",
        "2. 만약 확률변수 $Y$가 확률변수 $X$의 값을 독립변수로 하는 결정론적 함수값이라면,\r\n",
        "> $ Y = g(X)$\r\n",
        "3. 사용자가 $X$의 값을 어떤 값 $x$로 정하는 순간 $Y$의 값도 결정되어 버리기 때문에 $Y=g(X)$는 더이상 확률적인 값이 아니라 상수가 된다.\r\n",
        "> $\\text{E}[Y \\vert X] = \\text{E}[g(X) \\vert X] = g(X)$\r\n",
        "4. 같은 방식으로 확률변수 $X$와 $Y$가 결정론적 함수 관계가 아닐 때도 다음 등식이 성립한다.\r\n",
        "> $ \\text{E}[g(X) Y \\vert X] = g(X) \\text{E}[Y \\vert X]$\r\n",
        "\r\n",
        "<br/>\r\n",
        "\r\n",
        "### 4) 전체 기댓값의 법칙\r\n",
        "1. 조건부기댓값은 확률변수이므로 조건이 되는 확률변수에 대해 다시 기댓값을 구할 수 있다. 이렇게 반복하여 구한 조건부기댓값의 기댓값은 원래 확률변수의 댓값과 같다.\r\n",
        "> $ \\text{E}_X[\\text{E}_Y[Y \\vert X]] = \\text{E}_Y[Y]$   \r\n",
        "> 간단히,   \r\n",
        "> $ \\text{E}[\\text{E}[Y \\vert X]] = \\text{E}[Y]$\r\n",
        "2. 이를 전체 기댓값의 법칙(law of total expectation) 또는 반복 기댓값의 법칙(law of iterated expectation)이라  함\r\n",
        "3. $X, Y$가 이산확률변수인 경우에는 다음처럼 증명할 수 있다.\r\n",
        "> $\r\n",
        "\\begin{aligned}\r\n",
        "\\text{E}_X[\\text{E}_Y[Y|X]] \r\n",
        "&= \\sum_{x_i\\in X} p(x_i) \\text{E}_Y[Y|X] \\\\\r\n",
        "&= \\sum_{x_i\\in X} p(x_i) \\sum_{y_j \\in Y} p(y_j|x_i) y_j \\\\\r\n",
        "&= \\sum_{x_i\\in X} \\sum_{y_j \\in Y} p(x_i) p(y_j|x_i) y_j \\\\\r\n",
        "&= \\sum_{x_i\\in X} \\sum_{y_j \\in Y} p(x_i, y_j) y_j \\\\\r\n",
        "&= \\sum_{y_j \\in Y} p(y_j) y_j \\\\\r\n",
        "&= \\text{E}_Y[Y]\r\n",
        "\\end{aligned}\r\n",
        "$\r\n",
        "\r\n",
        "<br/>\r\n",
        "\r\n",
        "### 5) 조건부 분산\r\n",
        "1. 조건부분산(conditional variance) 정의\r\n",
        "> $ \r\n",
        "\\text{Var}_Y[Y \\vert X] \r\n",
        "= \\text{E}_Y[(Y - \\text{E}_Y[Y \\vert X])^2 \\vert X] \r\n",
        "= \\int (Y - \\text{E}_Y[Y \\vert X])^2 f_{Y \\vert X}(y \\vert x) dy $\r\n",
        "2. 조건부분산은 $x$의 값을 알고 있을 때 이에 대한 조건부확률분포 $p(y|x)$의 분산이다.\r\n",
        "3. 예측문제의 관점에서, 조건부분산 : 예측의 불확실성, 즉 예측으로 맞출 수 없는 범위\r\n",
        "\r\n",
        "<br/>\r\n",
        "\r\n",
        "### 6) 전체 분산의 법칙\r\n",
        "1. 전체 분산의 법칙(law of total variance) : 확률변수의 분산은 조건부분산의 기댓값과 조건부기댓값의 분산의 합과 같다.\r\n",
        "> $ \\text{Var}[Y] = \\text{E}[\\text{Var}[Y\\vert X]] + \\text{Var}[\\text{E}[Y\\vert X]]$\r\n",
        "2. 전체 기댓값의 법칙을 사용하여 증명\r\n",
        "> $\r\n",
        "\\begin{aligned}\r\n",
        "\\text{Var}[Y] \r\n",
        "&= \\text{E}[Y^2] - (\\text{E}[Y])^2 \\\\\r\n",
        "&= \\text{E} \\left[\\text{E}[Y^2\\mid X]\\right] - (\\text{E} [\\text{E}[Y\\mid X]])^2 \\\\\r\n",
        "&= \\text{E} \\left[\\text{Var}[Y\\mid X] + (\\text{E}[Y\\mid X])^2\\right] - (\\text{E}[\\text{E}[Y\\mid X]])^2 \\\\\r\n",
        "&= \\text{E} [\\text{Var}[Y\\mid X]] + \\left(\\text{E} [[\\text{E}[Y\\mid X]]^2] - (\\text{E} [\\text{E}[Y\\mid X]])^2\\right) \\\\\r\n",
        "&= \\text{E} [\\text{Var}[Y\\mid X]] + \\text{Var} [\\text{E}[Y\\mid X]]\r\n",
        "\\end{aligned}$   \r\n",
        "3. $\\text{E}[Y\\mid X] = \\hat{y}$로 표현하면 다음과 같이 쓸 수 있다.   \r\n",
        "> $\\text{Var}[Y] = \\text{E}[(\\hat{y}-y)^2] + \\text{Var}[\\hat{y}]$\r\n",
        "4. 예측문제의 관점에서 조건부분산의 기댓값 $\\text{E}[(\\hat{y}-y)^2]$은 예측 오차 즉, 편향(bias)의 평균적인 크기를 뜻한다.\r\n",
        "5. 조건부기댓값의 분산 $\\text{Var}[\\hat{y}]$ : 예측값의 변동 크기\r\n",
        "6. 예측값의 변동 크기가 증가한다는 것은 예측모형이 복잡하고 비선형적이며 주어진 데이터에 과최적화되기 쉽다는 의미\r\n",
        "> 따라서 전체 분산의 법칙이 말하고자 하는 바는 예측 오차의 크기과 예측값의 변동의 합이 일정하므로 예측 오차를 줄이면 모형이 복잡해지고 과최적화가 되며 반대로 모형을 과최적화를 막기위해 단순하게 하면 예측 오차가 증가한다. 이를 **편향-분산 상충(Bias–variance Tradeoff)** 법칙이라고도 한다."
      ]
    }
  ]
}
